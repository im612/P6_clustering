{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "yecThZFI6ndo"
   },
   "source": [
    "Ce notebook/fichier est le deuxième d'une série de quatre. Il présente des approches de sentence embedding appliqués aux texte du projet P6.\n",
    "Il résulte de l'adaptation d'un autre notebook fourni par Openclassrooms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "FI4DIgrX6ndz"
   },
   "source": [
    "# Préparation initiale dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "qeVY2vNc6nd1"
   },
   "source": [
    "## Récupération du dataset et filtres de données"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Préparation commune des traitements"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "VEsto9OI6nd3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: gensim in /usr/local/lib/python3.9/site-packages (4.2.0)\r\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.9/site-packages (from gensim) (1.7.2)\r\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/site-packages (from gensim) (6.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.9/site-packages (from gensim) (1.19.5)\r\n",
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "# Import des librairies\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import pickle\n",
    "import time\n",
    "from sklearn import cluster, metrics\n",
    "from sklearn import manifold, decomposition\n",
    "import logging\n",
    "\n",
    "# logging.disable(logging.WARNING) # disable WARNING, INFO and DEBUG logging everywhere\n",
    "# Import des librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# !{sys.executable} -m pip install tensorflow\n",
    "# !{sys.executable} -m pip install tensorflow_hub\n",
    "# !{sys.executable} -m pip install tensorflow_text\n",
    "!{sys.executable} -m pip install gensim"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "XlUwxsr16nd4",
    "outputId": "3b9bf5f3-15a1-47a4-809d-f625b2ce3cda"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lecture dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "weQqH0ud6nd8"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "EFryTjvW6nd-"
   },
   "source": [
    "## Fonctions communes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Calcul Tsne, détermination des clusters et calcul ARI entre vrais catégorie et n° de clusters\n",
    "def ARI_fct(features) :\n",
    "    time1 = time.time()\n",
    "    num_labels=len(l_cat)\n",
    "    tsne = manifold.TSNE(n_components=2, perplexity=30, n_iter=2000,\n",
    "                                 init='random', learning_rate=200, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(features)\n",
    "\n",
    "    # Détermination des clusters à partir des données après Tsne\n",
    "    cls = cluster.KMeans(n_clusters=num_labels, n_init=100, random_state=42)\n",
    "    cls.fit(X_tsne)\n",
    "    ARI = np.round(metrics.adjusted_rand_score(y_cat_num, cls.labels_),4)\n",
    "    time2 = np.round(time.time() - time1,0)\n",
    "    print(\"ARI : \", ARI, \"time : \", time2)\n",
    "\n",
    "    return ARI, X_tsne, cls.labels_\n",
    "\n",
    "\n",
    "# visualisation du Tsne selon les vraies catégories et selon les clusters\n",
    "def TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI) :\n",
    "    fig = plt.figure(figsize=(15,6))\n",
    "\n",
    "    ax = fig.add_subplot(121)\n",
    "    scatter = ax.scatter(X_tsne[:,0],X_tsne[:,1], c=y_cat_num, cmap='Set1')\n",
    "    ax.legend(handles=scatter.legend_elements()[0], labels=l_cat, loc=\"best\", title=\"Categorie\")\n",
    "    plt.title('Représentation des tweets par catégories réelles')\n",
    "\n",
    "    ax = fig.add_subplot(122)\n",
    "    scatter = ax.scatter(X_tsne[:,0],X_tsne[:,1], c=labels, cmap='Set1')\n",
    "    ax.legend(handles=scatter.legend_elements()[0], labels=set(labels), loc=\"best\", title=\"Clusters\")\n",
    "    plt.title('Représentation des tweets par clusters')\n",
    "\n",
    "    plt.show()\n",
    "    print(\"ARI : \", ARI)\n",
    "\n",
    "\n",
    "# df['first_category'] = df['product_category_tree'].str.extract(r'\\[\\\"(\\w* \\w*)')\n",
    "\n",
    "# Definizione l_cat e studio delle categorie\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "tZDFTT_b6nd_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                             uniq_id  crawl_timestamp  product_url  \\\nfirst_category                                                       \nBaby Care                        150              150          150   \nBeauty and Personal Care         150              150          150   \nComputers                        150              150          150   \nHome Decor & Festive Needs       150              150          150   \nHome Furnishing                  150              150          150   \nKitchen & Dining                 150              150          150   \nWatches                          150              150          150   \n\n                             product_name  product_category_tree  pid  \\\nfirst_category                                                          \nBaby Care                             150                    150  150   \nBeauty and Personal Care              150                    150  150   \nComputers                             150                    150  150   \nHome Decor & Festive Needs            150                    150  150   \nHome Furnishing                       150                    150  150   \nKitchen & Dining                      150                    150  150   \nWatches                               150                    150  150   \n\n                             retail_price  discounted_price  image  \\\nfirst_category                                                       \nBaby Care                             149               149    150   \nBeauty and Personal Care              150               150    150   \nComputers                             150               150    150   \nHome Decor & Festive Needs            150               150    150   \nHome Furnishing                       150               150    150   \nKitchen & Dining                      150               150    150   \nWatches                               150               150    150   \n\n                             is_FK_Advantage_product  description  \\\nfirst_category                                                      \nBaby Care                                        150          150   \nBeauty and Personal Care                         150          150   \nComputers                                        150          150   \nHome Decor & Festive Needs                       150          150   \nHome Furnishing                                  150          150   \nKitchen & Dining                                 150          150   \nWatches                                          150          150   \n\n                             product_rating  overall_rating  brand  \\\nfirst_category                                                       \nBaby Care                               150             150    134   \nBeauty and Personal Care                150             150     41   \nComputers                               150             150    150   \nHome Decor & Festive Needs              150             150    148   \nHome Furnishing                         150             150    150   \nKitchen & Dining                        150             150     79   \nWatches                                 150             150     10   \n\n                             product_specifications  second_category  \nfirst_category                                                        \nBaby Care                                       150              150  \nBeauty and Personal Care                        150              150  \nComputers                                       150              150  \nHome Decor & Festive Needs                      149              149  \nHome Furnishing                                 150              148  \nKitchen & Dining                                150              150  \nWatches                                         150              150  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>uniq_id</th>\n      <th>crawl_timestamp</th>\n      <th>product_url</th>\n      <th>product_name</th>\n      <th>product_category_tree</th>\n      <th>pid</th>\n      <th>retail_price</th>\n      <th>discounted_price</th>\n      <th>image</th>\n      <th>is_FK_Advantage_product</th>\n      <th>description</th>\n      <th>product_rating</th>\n      <th>overall_rating</th>\n      <th>brand</th>\n      <th>product_specifications</th>\n      <th>second_category</th>\n    </tr>\n    <tr>\n      <th>first_category</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Baby Care</th>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>149</td>\n      <td>149</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>134</td>\n      <td>150</td>\n      <td>150</td>\n    </tr>\n    <tr>\n      <th>Beauty and Personal Care</th>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>41</td>\n      <td>150</td>\n      <td>150</td>\n    </tr>\n    <tr>\n      <th>Computers</th>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n    </tr>\n    <tr>\n      <th>Home Decor &amp; Festive Needs</th>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>148</td>\n      <td>149</td>\n      <td>149</td>\n    </tr>\n    <tr>\n      <th>Home Furnishing</th>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>148</td>\n    </tr>\n    <tr>\n      <th>Kitchen &amp; Dining</th>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>79</td>\n      <td>150</td>\n      <td>150</td>\n    </tr>\n    <tr>\n      <th>Watches</th>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>150</td>\n      <td>10</td>\n      <td>150</td>\n      <td>150</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catégories :  ['Baby Care ', 'Beauty and Personal Care ', 'Computers ', 'Home Decor & Festive Needs ', 'Home Furnishing ', 'Kitchen & Dining ', 'Watches ']\n",
      "[4, 0, 0, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4, 4, 4, 3, 5, 5, 4, 0, 4, 0, 1, 5, 5, 5, 2, 5, 1, 5, 2, 5, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 0, 5, 5, 0, 4, 5, 5, 5, 4, 5, 0, 0, 0, 1, 1, 4, 0, 3, 3, 0, 0, 3, 3, 5, 2, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 4, 4, 4, 0, 4, 4, 4, 0, 3, 0, 5, 0, 2, 3, 0, 3, 2, 4, 0, 2, 3, 1, 1, 1, 1, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 5, 3, 5, 3, 3, 0, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 3, 5, 3, 0, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 3, 3, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 3, 5, 3, 5, 5, 5, 3, 5, 3, 3, 3, 5, 3, 5, 3, 3, 3, 3, 5, 5, 5, 3, 3, 3, 3, 5, 5, 0, 6, 6, 6, 6, 6, 4, 0, 6, 3, 6, 6, 4, 0, 3, 3, 0, 0, 0, 0, 5, 2, 0, 5, 3, 5, 1, 3, 3, 3, 3, 3, 3, 3, 3, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 2, 0, 2, 0, 2, 5, 2, 2, 2, 2, 2, 3, 1, 0, 4, 0, 1, 0, 4, 4, 2, 2, 2, 0, 0, 0, 4, 5, 4, 5, 5, 4, 4, 2, 1, 0, 5, 4, 1, 0, 0, 0, 0, 0, 5, 1, 5, 5, 1, 5, 1, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 2, 6, 2, 2, 2, 6, 6, 2, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 2, 0, 4, 0, 0, 3, 3, 3, 0, 2, 0, 3, 5, 6, 5, 2, 2, 1, 1, 1, 3, 3, 4, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3, 4, 0, 6, 4, 0, 0, 0, 0, 3, 4, 4, 3, 4, 4, 0, 0, 0, 0, 5, 0, 0, 5, 0, 5, 4, 0, 4, 5, 0, 3, 5, 0, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 3, 3, 3, 3, 3, 4, 4, 4, 4, 3, 4, 4, 4, 3, 4, 4, 0, 3, 4, 0, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 3, 0, 3, 4, 3, 1, 4, 0, 2, 3, 1, 1, 1, 3, 5, 2, 6, 5, 5, 5, 5, 4, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 6, 4, 6, 6, 6, 6, 6, 6, 5, 5, 1, 1, 6, 0, 0, 0, 0, 0, 0, 0, 6, 1, 1, 6, 6, 6, 0, 0, 0, 6, 6, 6, 0, 6, 1, 1, 0, 1, 6, 6, 1, 1, 4, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 3, 3, 4, 4, 4, 1, 4, 3, 6, 4, 6, 6, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 0, 1, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 1, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 1, 3, 1, 3, 5, 3, 0, 3, 3, 0, 0, 0, 0, 3, 3, 6, 0, 0, 3, 3, 4, 3, 4, 3, 3, 3, 1, 3, 4, 4, 4, 3, 3, 4, 4, 3, 4, 1, 3, 3, 3, 4, 3, 4, 4, 3, 3, 4, 1, 3, 3, 4, 4, 3, 4, 4, 4, 3, 5, 3, 4, 4, 4, 4, 4, 4, 4, 1, 5, 0, 0, 4, 0, 0, 0, 3, 3, 0, 1, 0, 5, 0, 0, 0, 0, 0, 3, 1, 3, 3, 1, 3, 0, 5, 5, 4, 5, 0, 0, 3, 5, 4, 5, 0, 2, 5, 0, 1, 1, 1, 5, 3, 0, 0, 3, 1, 0, 2, 1, 4, 2, 2, 4, 2, 4, 2, 2, 2, 4, 4, 2, 4, 1, 1, 2, 4, 2, 2, 2, 4, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "nom_source = \"data/source/Flipkart/flipkart_com-ecommerce_sample_1050.csv\"\n",
    "df = pd.read_csv(nom_source, sep= ',', low_memory=False )\n",
    "\n",
    "df['first_category'] = df['product_category_tree'].str.extract(r'^\\[\\\"(.+?)[\\>\\>]')\n",
    "df['second_category'] = df['product_category_tree'].str.extract(r'[\\>\\>](.+?)[\\>\\>]').replace(r'\\> ', '', regex=True)\n",
    "\n",
    "df.head()\n",
    "\n",
    "# Étude des catégories\n",
    "cat_N1 = df.groupby(by='first_category').count().index.to_list()\n",
    "display(df.groupby(by='first_category').count())\n",
    "\n",
    "cat_N2 = df.groupby(by='second_category').count().index.to_list()\n",
    "# display(len(df.groupby(by='second_category').count().index.to_list()))\n",
    "\n",
    "l_cat = cat_N1\n",
    "\n",
    "print(\"catégories : \", l_cat)\n",
    "# y_cat_num = [(1-l_cat.index(df.iloc[i]['first_category'])) for i in range(len(df))]\n",
    "y_cat_num = [(l_cat.index(df.iloc[i]['first_category'])) for i in range(len(df))]\n",
    "print(y_cat_num)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "jLPBaaQM6neB",
    "outputId": "1f77d313-1e23-4f19-de65-78fa44f52f73"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word2Vec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "H02dJJSv6neD"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "NO-XNcqU6neE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import metrics as kmetrics\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "cPXRvoJs6neF"
   },
   "source": [
    "## Création du modèle Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Tokenizer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "\n",
    "nom_source = \"data/source/Flipkart/flipkart_com-ecommerce_sample_1050.csv\"\n",
    "df = pd.read_csv(nom_source, sep= ',', low_memory=False )\n",
    "\n",
    "\n",
    "def tokenizer_fct(sentence) :\n",
    "    # print(sentence)\n",
    "    sentence_clean = sentence.replace('-', ' ').replace('+', ' ').replace('/', ' ').replace('#', ' ')\n",
    "    word_tokens = word_tokenize(sentence_clean)\n",
    "    return word_tokens\n",
    "\n",
    "# Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_w = list(set(stopwords.words('english'))) + ['[', ']', ',', '.', ':', '?', '(', ')', '-']\n",
    "\n",
    "stopwords_specific = ['pack', 'set', 'combo', 'box',\n",
    "                      'jaipur', 'elegance', 'shape',  'print',  'light', 'led',\n",
    "                      'rockmantra', 'eurospa', 'terry',\n",
    "                     'printed','usb', 'print',\n",
    "                     'double', 'single',\n",
    "                     'red', 'brown', 'black', 'multicolor', 'blue', 'color', 'green',\n",
    "                      'abstract', 'floral',\n",
    "                     'vinyl', 'ideal',\n",
    "                     'large', 'comfort', 'extra', 'sized', 'height', 'width', 'lenght',\n",
    "                     'polyester', 'crystal', 'ceramic', 'paper', 'cotton', 'porcelain',\n",
    "                     'lapguard', 'sstudio', 'sonata', 'vgn', 'vaio',\n",
    "                     'gathered', 'printland', 'prithish', 'hot', 'product','maximum']\n",
    "\n",
    "stop_w = stop_w + stopwords_specific\n",
    "\n",
    "def stop_word_filter_fct(list_words) :\n",
    "    filtered_w = [w for w in list_words if not w in stop_w]\n",
    "    filtered_w2 = [w for w in filtered_w if len(w) > 2]\n",
    "    return filtered_w2\n",
    "\n",
    "# lower case et alpha\n",
    "def lower_start_fct(list_words) :\n",
    "    lw = [w.lower() for w in list_words if (not w.startswith(\"@\"))]\n",
    "    #                                   and (not w.startswith(\"#\"))\n",
    "    #                                    and (not w.startswith(\"http\"))]\n",
    "    return lw\n",
    "\n",
    "# Lemmatizer (base d'un mot)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemma_fct(list_words) :\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_w = [lemmatizer.lemmatize(w) for w in list_words]\n",
    "    return lem_w\n",
    "\n",
    "# Fonction de préparation du texte pour le bag of words (Countvectorizer et Tf_idf, Word2Vec)\n",
    "def transform_bow_fct(desc_text) :\n",
    "    word_tokens = tokenizer_fct(desc_text)\n",
    "    sw = stop_word_filter_fct(word_tokens)\n",
    "    lw = lower_start_fct(sw)\n",
    "    # lem_w = lemma_fct(lw)\n",
    "    transf_desc_text = ' '.join(lw)\n",
    "    return transf_desc_text\n",
    "\n",
    "# Fonction de préparation du texte pour le bag of words avec lemmatization\n",
    "def transform_bow_lem_fct(desc_text) :\n",
    "    word_tokens = tokenizer_fct(desc_text)\n",
    "    sw = stop_word_filter_fct(word_tokens)\n",
    "    lw = lower_start_fct(sw)\n",
    "    lem_w = lemma_fct(lw)\n",
    "    transf_desc_text = ' '.join(lem_w)\n",
    "    return transf_desc_text\n",
    "\n",
    "# Fonction de préparation du texte pour le Deep learning (USE et BERT)\n",
    "def transform_dl_fct(desc_text) :\n",
    "    word_tokens = tokenizer_fct(desc_text)\n",
    "#    sw = stop_word_filter_fct(word_tokens)\n",
    "    lw = lower_start_fct(word_tokens)\n",
    "    # lem_w = lemma_fct(lw)\n",
    "    transf_desc_text = ' '.join(lw)\n",
    "    return transf_desc_text\n",
    "\n",
    "\n",
    "\n",
    "# Création d'un texte extrait de la description par l'intersection avec le nom du produit\n",
    "def transform_descr_new() :\n",
    "    s_i = []\n",
    "\n",
    "    for i in range(len((df['description'].values))):\n",
    "    # j = str().split(\" \")\n",
    "    # n = str(df['product_name'].values[i]).replace('-', '').split(\" \")\n",
    "\n",
    "        j = tokenizer_fct(df['description'].values[i])\n",
    "        j = lower_start_fct(j)\n",
    "        n = tokenizer_fct(df['product_name'].values[i])\n",
    "        n = lower_start_fct(n)\n",
    "\n",
    "        for w in n :\n",
    "            # print(w, len(w))\n",
    "            if not w.isalpha() or len(w)<3:\n",
    "                n.remove(w)\n",
    "\n",
    "        if '' in n:\n",
    "            n.remove('')\n",
    "\n",
    "        # print(n)\n",
    "        s = ''\n",
    "        for w in j:\n",
    "            # print(w)\n",
    "            if w in n and w not in stopwords_specific:\n",
    "                # print(w, 'ok')\n",
    "                s = s + ' ' + w\n",
    "\n",
    "        s_i.append(s)\n",
    "\n",
    "    df['descr_new'] = pd.Series(s_i)\n",
    "\n",
    "transform_descr_new()\n",
    "\n",
    "df.head()\n",
    "\n",
    "feats = ['description', 'product_name', 'descr_new']\n",
    "\n",
    "feat_start = feats[2]\n",
    "\n",
    "df['sentence_bow'] = df[feat_start].apply(lambda x : transform_bow_fct(x))\n",
    "df['sentence_bow_lem'] = df[feat_start].apply(lambda x : transform_bow_lem_fct(x))\n",
    "df['sentence_dl'] = df[feat_start].apply(lambda x : transform_dl_fct(x))\n",
    "df['len_d'] = df['sentence_bow'].apply(len)+df['sentence_bow_lem'].apply(len)+(df['sentence_dl']).apply(len)\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "42yzRAqs6neG",
    "outputId": "dfcc8acc-9cac-4e42-b144-00bb8cf92eb2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "import gensim\n",
    "\n",
    "w2v_size=300\n",
    "w2v_window=5\n",
    "w2v_min_count=1\n",
    "w2v_epochs=100\n",
    "maxlen = 24 # adapt to length of sentences\n",
    "# sentences = data_T['sentence_bow_lem'].to_list()\n",
    "sentences = df['sentence_bow_lem'].to_list()\n",
    "sentences = [gensim.utils.simple_preprocess(text) for text in sentences]"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "0jctzpOr6neI"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "hoOHysOs6neJ",
    "outputId": "160eb8df-bc8c-434e-c6a2-b1ee9d9e1031"
   },
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "# Création et entraînement du modèle Word2Vec\n",
    "\n",
    "print(\"Build & train Word2Vec model ...\")\n",
    "w2v_model = gensim.models.Word2Vec(min_count=w2v_min_count, window=w2v_window,\n",
    "                                                vector_size=w2v_size,\n",
    "                                                seed=42,\n",
    "                                                workers=1)\n",
    "#                                                workers=multiprocessing.cpu_count())\n",
    "w2v_model.build_vocab(sentences)\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=w2v_epochs)\n",
    "model_vectors = w2v_model.wv\n",
    "w2v_words = model_vectors.index_to_key\n",
    "print(\"Vocabulary size: %i\" % len(w2v_words))\n",
    "print(\"Word2Vec trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "STRQfVqP6neJ"
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Préparation des sentences (tokenization)\n",
    "\n",
    "print(\"Fit Tokenizer ...\")\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "x_sentences = pad_sequences(tokenizer.texts_to_sequences(sentences),\n",
    "                                                     maxlen=maxlen,\n",
    "                                                     padding='post') \n",
    "                                                   \n",
    "num_words = len(tokenizer.word_index) + 1\n",
    "print(\"Number of unique words: %i\" % num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "2ZcSsjRI6neK"
   },
   "source": [
    "## Création de la matrice d'embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "h9nu2DC36neL"
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Création de la matrice d'embedding\n",
    "\n",
    "print(\"Create Embedding matrix ...\")\n",
    "w2v_size = 300\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, w2v_size))\n",
    "i=0\n",
    "j=0\n",
    "    \n",
    "for word, idx in word_index.items():\n",
    "    i +=1\n",
    "    if word in w2v_words:\n",
    "        j +=1\n",
    "        embedding_vector = model_vectors[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = model_vectors[word]\n",
    "            \n",
    "word_rate = np.round(j/i,4)\n",
    "print(\"Word embedding rate : \", word_rate)\n",
    "print(\"Embedding matrix: %s\" % str(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "w843Ct1h6neW"
   },
   "source": [
    "## Création du modèle d'embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "F3r5_G4L6neX"
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Création du modèle\n",
    "\n",
    "input=Input(shape=(len(x_sentences),maxlen),dtype='float64')\n",
    "word_input=Input(shape=(maxlen,),dtype='float64')  \n",
    "word_embedding=Embedding(input_dim=vocab_size,\n",
    "                         output_dim=w2v_size,\n",
    "                         weights = [embedding_matrix],\n",
    "                         input_length=maxlen)(word_input)\n",
    "word_vec=GlobalAveragePooling1D()(word_embedding)  \n",
    "embed_model = Model([word_input],word_vec)\n",
    "\n",
    "embed_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "YTxt0fwh6neY"
   },
   "source": [
    "## Exécution du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "Is7DBCML6neY"
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "embeddings = embed_model.predict(x_sentences)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "ARI, X_tsne, labels = ARI_fct(embeddings)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "AP85QY3o6neZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "54oLZzsf6nea"
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "rEy4hgez6nea"
   },
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "BvBZhFpX6neb"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import metrics as kmetrics\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Bert\n",
    "!{sys.executable} -m pip install transformers\n",
    "\n",
    "import os\n",
    "import transformers\n",
    "from transformers import *\n",
    "\n",
    "os.environ[\"TF_KERAS\"]='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "7uaYXvmJ6neb"
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(tensorflow.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "yS1BCv3Z6neb"
   },
   "source": [
    "## Fonctions communes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "SqKt2zLF6neb"
   },
   "outputs": [],
   "source": [
    "# Fonction de préparation des sentences\n",
    "def bert_inp_fct(sentences, bert_tokenizer, max_length) :\n",
    "    input_ids=[]\n",
    "    token_type_ids = []\n",
    "    attention_mask=[]\n",
    "    bert_inp_tot = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        bert_inp = bert_tokenizer.encode_plus(sent,\n",
    "                                              add_special_tokens = True,\n",
    "                                              max_length = max_length,\n",
    "                                              padding='max_length',\n",
    "                                              return_attention_mask = True, \n",
    "                                              return_token_type_ids=True,\n",
    "                                              truncation=True,\n",
    "                                              return_tensors=\"tf\")\n",
    "    \n",
    "        input_ids.append(bert_inp['input_ids'][0])\n",
    "        token_type_ids.append(bert_inp['token_type_ids'][0])\n",
    "        attention_mask.append(bert_inp['attention_mask'][0])\n",
    "        bert_inp_tot.append((bert_inp['input_ids'][0], \n",
    "                             bert_inp['token_type_ids'][0], \n",
    "                             bert_inp['attention_mask'][0]))\n",
    "\n",
    "    input_ids = np.asarray(input_ids)\n",
    "    token_type_ids = np.asarray(token_type_ids)\n",
    "    attention_mask = np.array(attention_mask)\n",
    "    \n",
    "    return input_ids, token_type_ids, attention_mask, bert_inp_tot\n",
    "    \n",
    "\n",
    "# Fonction de création des features\n",
    "def feature_BERT_fct(model, model_type, sentences, max_length, b_size, mode='HF') :\n",
    "    batch_size = b_size\n",
    "    batch_size_pred = b_size\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "    time1 = time.time()\n",
    "\n",
    "    for step in range(len(sentences)//batch_size) :\n",
    "        idx = step*batch_size\n",
    "        input_ids, token_type_ids, attention_mask, bert_inp_tot = bert_inp_fct(sentences[idx:idx+batch_size], \n",
    "                                                                      bert_tokenizer, max_length)\n",
    "        \n",
    "        if mode=='HF' :    # Bert HuggingFace\n",
    "            outputs = model.predict([input_ids, attention_mask, token_type_ids], batch_size=batch_size_pred)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        if mode=='TFhub' : # Bert Tensorflow Hub\n",
    "            text_preprocessed = {\"input_word_ids\" : input_ids, \n",
    "                                 \"input_mask\" : attention_mask, \n",
    "                                 \"input_type_ids\" : token_type_ids}\n",
    "            outputs = model(text_preprocessed)\n",
    "            last_hidden_states = outputs['sequence_output']\n",
    "             \n",
    "        if step ==0 :\n",
    "            last_hidden_states_tot = last_hidden_states\n",
    "            last_hidden_states_tot_0 = last_hidden_states\n",
    "        else :\n",
    "            last_hidden_states_tot = np.concatenate((last_hidden_states_tot,last_hidden_states))\n",
    "    \n",
    "    features_bert = np.array(last_hidden_states_tot).mean(axis=1)\n",
    "    \n",
    "    time2 = np.round(time.time() - time1,0)\n",
    "    print(\"temps traitement : \", time2)\n",
    "     \n",
    "    return features_bert, last_hidden_states_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "lUSPnqgZ6nec"
   },
   "source": [
    "## BERT HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "DjYcPLqm6ned"
   },
   "source": [
    "### 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "rqu2jvmH6ned"
   },
   "outputs": [],
   "source": [
    "max_length = 64\n",
    "batch_size = 10\n",
    "model_type = 'bert-base-uncased'\n",
    "model = TFAutoModel.from_pretrained(model_type)\n",
    "sentences = df['sentence_dl'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "pTtTDMiO6nee"
   },
   "outputs": [],
   "source": [
    "# Création des features\n",
    "\n",
    "features_bert, last_hidden_states_tot = feature_BERT_fct(model, model_type, sentences, \n",
    "                                                         max_length, batch_size, mode='HF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "edgE-axf6nef"
   },
   "outputs": [],
   "source": [
    "ARI, X_tsne, labels = ARI_fct(features_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "raOFIbbO6nef"
   },
   "outputs": [],
   "source": [
    "TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ari_bert_base_uncased = ARI"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "6-GH6Mla6nef"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "ih0MkZPM6neg"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "TmEM_EcV6neg"
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow_text \n",
    "\n",
    "# Guide sur le Tensorflow hub : https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n",
    "model_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4'\n",
    "bert_layer = hub.KerasLayer(model_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "KUAQadCR6neh"
   },
   "outputs": [],
   "source": [
    "sentences = df['sentence_dl'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "5of1kgca6neh"
   },
   "outputs": [],
   "source": [
    "max_length = 64\n",
    "batch_size = 10\n",
    "model_type = 'bert-base-uncased'\n",
    "model = bert_layer\n",
    "\n",
    "features_bert, last_hidden_states_tot = feature_BERT_fct(model, model_type, sentences, \n",
    "                                                         max_length, batch_size, mode='TFhub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "U3dD6tWD6neh"
   },
   "outputs": [],
   "source": [
    "ARI, X_tsne, labels = ARI_fct(features_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "tMd6KPo16nej"
   },
   "outputs": [],
   "source": [
    "TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ari_hub_tensorflow = ARI"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "8ZPDCa8D6nej"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "mDotHtTS6nek"
   },
   "source": [
    "# USE - Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "gUx6dYhy6nek"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import metrics as kmetrics\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Bert\n",
    "import transformers\n",
    "from transformers import *\n",
    "\n",
    "os.environ[\"TF_KERAS\"]='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "yR3hP6bA6nek"
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(tensorflow.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "zK0Mfl9Z6nel"
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "b9RCDZFh6nem"
   },
   "outputs": [],
   "source": [
    "def feature_USE_fct(sentences, b_size) :\n",
    "    batch_size = b_size\n",
    "    time1 = time.time()\n",
    "\n",
    "    for step in range(len(sentences)//batch_size) :\n",
    "        idx = step*batch_size\n",
    "        feat = embed(sentences[idx:idx+batch_size])\n",
    "\n",
    "        if step ==0 :\n",
    "            features = feat\n",
    "        else :\n",
    "            features = np.concatenate((features,feat))\n",
    "\n",
    "    time2 = np.round(time.time() - time1,0)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "AQw2EpDL6nem"
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "sentences = df['sentence_dl'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "4A0U9-1M6nem"
   },
   "outputs": [],
   "source": [
    "features_USE = feature_USE_fct(sentences, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "idu18MQB6nen"
   },
   "outputs": [],
   "source": [
    "ARI, X_tsne, labels = ARI_fct(features_USE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "_zYwdmzT6nen"
   },
   "outputs": [],
   "source": [
    "TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "F02cJNbV6neo"
   },
   "outputs": [],
   "source": [
    "ari_use = ARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('ARI BERT base uncased: %s, ARI hub tensorflow: %s, ARI USE: %s' % (ari_bert_base_uncased, ari_hub_tensorflow, ari_use))\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "dinEWD3p6neo"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "colab": {
   "name": "MEREU_Ilaria_1_2_notebook_sentence-embedding-juillet-2022.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "DjYcPLqm6ned"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}