# -*- coding: utf-8 -*-
"""MEREU_Ilaria_1_2_notebook_sentence-embedding-juillet-2022.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gGxoG1zB7gN-QPlNIcDF7C682hUO14CA

Ce notebook/fichier est le deuxième d'une série de quatre. Il présente des approches de sentence embedding appliqués aux texte du projet P6.
Il résulte de l'adaptation d'un autre notebook fourni par Openclassrooms.

# Préparation initiale dataset

## Récupération du dataset et filtres de données

# Préparation commune des traitements
"""

# Import des librairies
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import pickle
import time
from sklearn import cluster, metrics
from sklearn import manifold, decomposition
import logging

# logging.disable(logging.WARNING) # disable WARNING, INFO and DEBUG logging everywhere
# Import des librairies
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# !{sys.executable} -m pip install tensorflow
# !{sys.executable} -m pip install tensorflow_hub
# !{sys.executable} -m pip install tensorflow_text
!{sys.executable} -m pip install gensim

"""## Lecture dataset

## Fonctions communes
"""

import time

# Calcul Tsne, détermination des clusters et calcul ARI entre vrais catégorie et n° de clusters
def ARI_fct(features) :
    time1 = time.time()
    num_labels=len(l_cat)
    tsne = manifold.TSNE(n_components=2, perplexity=30, n_iter=2000,
                                 init='random', learning_rate=200, random_state=42)
    X_tsne = tsne.fit_transform(features)

    # Détermination des clusters à partir des données après Tsne
    cls = cluster.KMeans(n_clusters=num_labels, n_init=100, random_state=42)
    cls.fit(X_tsne)
    ARI = np.round(metrics.adjusted_rand_score(y_cat_num, cls.labels_),4)
    time2 = np.round(time.time() - time1,0)
    print("ARI : ", ARI, "time : ", time2)

    return ARI, X_tsne, cls.labels_


# visualisation du Tsne selon les vraies catégories et selon les clusters
def TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI) :
    fig = plt.figure(figsize=(15,6))

    ax = fig.add_subplot(121)
    scatter = ax.scatter(X_tsne[:,0],X_tsne[:,1], c=y_cat_num, cmap='Set1')
    ax.legend(handles=scatter.legend_elements()[0], labels=l_cat, loc="best", title="Categorie")
    plt.title('Représentation des tweets par catégories réelles')

    ax = fig.add_subplot(122)
    scatter = ax.scatter(X_tsne[:,0],X_tsne[:,1], c=labels, cmap='Set1')
    ax.legend(handles=scatter.legend_elements()[0], labels=set(labels), loc="best", title="Clusters")
    plt.title('Représentation des tweets par clusters')

    plt.show()
    print("ARI : ", ARI)


# df['first_category'] = df['product_category_tree'].str.extract(r'\[\"(\w* \w*)')

# Definizione l_cat e studio delle categorie

nom_source = "data/source/Flipkart/flipkart_com-ecommerce_sample_1050.csv"
df = pd.read_csv(nom_source, sep= ',', low_memory=False )

df['first_category'] = df['product_category_tree'].str.extract(r'^\[\"(.+?)[\>\>]')
df['second_category'] = df['product_category_tree'].str.extract(r'[\>\>](.+?)[\>\>]').replace(r'\> ', '', regex=True)

df.head()

# Étude des catégories
cat_N1 = df.groupby(by='first_category').count().index.to_list()
display(df.groupby(by='first_category').count())

cat_N2 = df.groupby(by='second_category').count().index.to_list()
# display(len(df.groupby(by='second_category').count().index.to_list()))

l_cat = cat_N1

print("catégories : ", l_cat)
# y_cat_num = [(1-l_cat.index(df.iloc[i]['first_category'])) for i in range(len(df))]
y_cat_num = [(l_cat.index(df.iloc[i]['first_category'])) for i in range(len(df))]
print(y_cat_num)

"""# Word2Vec"""

import tensorflow as tf
import tensorflow.keras
from tensorflow.keras import backend as K

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import metrics as kmetrics
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
import gensim

"""## Création du modèle Word2Vec"""

import pandas as pd
# Tokenizer
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
import re

nom_source = "data/source/Flipkart/flipkart_com-ecommerce_sample_1050.csv"
df = pd.read_csv(nom_source, sep= ',', low_memory=False )


def tokenizer_fct(sentence) :
    # print(sentence)
    sentence_clean = sentence.replace('-', ' ').replace('+', ' ').replace('/', ' ').replace('#', ' ')
    word_tokens = word_tokenize(sentence_clean)
    return word_tokens

# Stop words
from nltk.corpus import stopwords
stop_w = list(set(stopwords.words('english'))) + ['[', ']', ',', '.', ':', '?', '(', ')', '-']

stopwords_specific = ['pack', 'set', 'combo', 'box',
                      'jaipur', 'elegance', 'shape',  'print',  'light', 'led',
                      'rockmantra', 'eurospa', 'terry',
                     'printed','usb', 'print',
                     'double', 'single',
                     'red', 'brown', 'black', 'multicolor', 'blue', 'color', 'green',
                      'abstract', 'floral',
                     'vinyl', 'ideal',
                     'large', 'comfort', 'extra', 'sized', 'height', 'width', 'lenght',
                     'polyester', 'crystal', 'ceramic', 'paper', 'cotton', 'porcelain',
                     'lapguard', 'sstudio', 'sonata', 'vgn', 'vaio',
                     'gathered', 'printland', 'prithish', 'hot', 'product','maximum']

stop_w = stop_w + stopwords_specific

def stop_word_filter_fct(list_words) :
    filtered_w = [w for w in list_words if not w in stop_w]
    filtered_w2 = [w for w in filtered_w if len(w) > 2]
    return filtered_w2

# lower case et alpha
def lower_start_fct(list_words) :
    lw = [w.lower() for w in list_words if (not w.startswith("@"))]
    #                                   and (not w.startswith("#"))
    #                                    and (not w.startswith("http"))]
    return lw

# Lemmatizer (base d'un mot)
from nltk.stem import WordNetLemmatizer

def lemma_fct(list_words) :
    lemmatizer = WordNetLemmatizer()
    lem_w = [lemmatizer.lemmatize(w) for w in list_words]
    return lem_w

# Fonction de préparation du texte pour le bag of words (Countvectorizer et Tf_idf, Word2Vec)
def transform_bow_fct(desc_text) :
    word_tokens = tokenizer_fct(desc_text)
    sw = stop_word_filter_fct(word_tokens)
    lw = lower_start_fct(sw)
    # lem_w = lemma_fct(lw)
    transf_desc_text = ' '.join(lw)
    return transf_desc_text

# Fonction de préparation du texte pour le bag of words avec lemmatization
def transform_bow_lem_fct(desc_text) :
    word_tokens = tokenizer_fct(desc_text)
    sw = stop_word_filter_fct(word_tokens)
    lw = lower_start_fct(sw)
    lem_w = lemma_fct(lw)
    transf_desc_text = ' '.join(lem_w)
    return transf_desc_text

# Fonction de préparation du texte pour le Deep learning (USE et BERT)
def transform_dl_fct(desc_text) :
    word_tokens = tokenizer_fct(desc_text)
#    sw = stop_word_filter_fct(word_tokens)
    lw = lower_start_fct(word_tokens)
    # lem_w = lemma_fct(lw)
    transf_desc_text = ' '.join(lw)
    return transf_desc_text



# Création d'un texte extrait de la description par l'intersection avec le nom du produit
def transform_descr_new() :
    s_i = []

    for i in range(len((df['description'].values))):
    # j = str().split(" ")
    # n = str(df['product_name'].values[i]).replace('-', '').split(" ")

        j = tokenizer_fct(df['description'].values[i])
        j = lower_start_fct(j)
        n = tokenizer_fct(df['product_name'].values[i])
        n = lower_start_fct(n)

        for w in n :
            # print(w, len(w))
            if not w.isalpha() or len(w)<3:
                n.remove(w)

        if '' in n:
            n.remove('')

        # print(n)
        s = ''
        for w in j:
            # print(w)
            if w in n and w not in stopwords_specific:
                # print(w, 'ok')
                s = s + ' ' + w

        s_i.append(s)

    df['descr_new'] = pd.Series(s_i)

transform_descr_new()

df.head()

feats = ['description', 'product_name', 'descr_new']

feat_start = feats[2]

df['sentence_bow'] = df[feat_start].apply(lambda x : transform_bow_fct(x))
df['sentence_bow_lem'] = df[feat_start].apply(lambda x : transform_bow_lem_fct(x))
df['sentence_dl'] = df[feat_start].apply(lambda x : transform_dl_fct(x))
df['len_d'] = df['sentence_bow'].apply(len)+df['sentence_bow_lem'].apply(len)+(df['sentence_dl']).apply(len)

df.head()

# %%script false --no-raise-error
import gensim

w2v_size=300
w2v_window=5
w2v_min_count=1
w2v_epochs=100
maxlen = 24 # adapt to length of sentences
# sentences = data_T['sentence_bow_lem'].to_list()
sentences = df['sentence_bow_lem'].to_list()
sentences = [gensim.utils.simple_preprocess(text) for text in sentences]

# %%script false --no-raise-error
# Création et entraînement du modèle Word2Vec

print("Build & train Word2Vec model ...")
w2v_model = gensim.models.Word2Vec(min_count=w2v_min_count, window=w2v_window,
                                                vector_size=w2v_size,
                                                seed=42,
                                                workers=1)
#                                                workers=multiprocessing.cpu_count())
w2v_model.build_vocab(sentences)
w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=w2v_epochs)
model_vectors = w2v_model.wv
w2v_words = model_vectors.index_to_key
print("Vocabulary size: %i" % len(w2v_words))
print("Word2Vec trained")

# Commented out IPython magic to ensure Python compatibility.
# %%script false --no-raise-error
# # Préparation des sentences (tokenization)
# 
# print("Fit Tokenizer ...")
# tokenizer = Tokenizer()
# tokenizer.fit_on_texts(sentences)
# x_sentences = pad_sequences(tokenizer.texts_to_sequences(sentences),
#                                                      maxlen=maxlen,
#                                                      padding='post') 
#                                                    
# num_words = len(tokenizer.word_index) + 1
# print("Number of unique words: %i" % num_words)

"""## Création de la matrice d'embedding"""

# Commented out IPython magic to ensure Python compatibility.
# %%script false --no-raise-error
# # Création de la matrice d'embedding
# 
# print("Create Embedding matrix ...")
# w2v_size = 300
# word_index = tokenizer.word_index
# vocab_size = len(word_index) + 1
# embedding_matrix = np.zeros((vocab_size, w2v_size))
# i=0
# j=0
#     
# for word, idx in word_index.items():
#     i +=1
#     if word in w2v_words:
#         j +=1
#         embedding_vector = model_vectors[word]
#         if embedding_vector is not None:
#             embedding_matrix[idx] = model_vectors[word]
#             
# word_rate = np.round(j/i,4)
# print("Word embedding rate : ", word_rate)
# print("Embedding matrix: %s" % str(embedding_matrix.shape))

"""## Création du modèle d'embedding"""

# Commented out IPython magic to ensure Python compatibility.
# %%script false --no-raise-error
# # Création du modèle
# 
# input=Input(shape=(len(x_sentences),maxlen),dtype='float64')
# word_input=Input(shape=(maxlen,),dtype='float64')  
# word_embedding=Embedding(input_dim=vocab_size,
#                          output_dim=w2v_size,
#                          weights = [embedding_matrix],
#                          input_length=maxlen)(word_input)
# word_vec=GlobalAveragePooling1D()(word_embedding)  
# embed_model = Model([word_input],word_vec)
# 
# embed_model.summary()

"""## Exécution du modèle"""

# Commented out IPython magic to ensure Python compatibility.
# %%script false --no-raise-error
# embeddings = embed_model.predict(x_sentences)
# embeddings.shape

# Commented out IPython magic to ensure Python compatibility.
# %%script false --no-raise-error
# ARI, X_tsne, labels = ARI_fct(embeddings)

# Commented out IPython magic to ensure Python compatibility.
# %%script false --no-raise-error
# TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI)

"""# BERT"""

import tensorflow as tf
# import tensorflow_hub as hub
import tensorflow.keras
from tensorflow.keras import backend as K

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import metrics as kmetrics
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model

# Bert
!{sys.executable} -m pip install transformers

import os
import transformers
from transformers import *

os.environ["TF_KERAS"]='1'

print(tf.__version__)
print(tensorflow.__version__)
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
print(tf.test.is_built_with_cuda())

"""## Fonctions communes"""

# Fonction de préparation des sentences
def bert_inp_fct(sentences, bert_tokenizer, max_length) :
    input_ids=[]
    token_type_ids = []
    attention_mask=[]
    bert_inp_tot = []

    for sent in sentences:
        bert_inp = bert_tokenizer.encode_plus(sent,
                                              add_special_tokens = True,
                                              max_length = max_length,
                                              padding='max_length',
                                              return_attention_mask = True, 
                                              return_token_type_ids=True,
                                              truncation=True,
                                              return_tensors="tf")
    
        input_ids.append(bert_inp['input_ids'][0])
        token_type_ids.append(bert_inp['token_type_ids'][0])
        attention_mask.append(bert_inp['attention_mask'][0])
        bert_inp_tot.append((bert_inp['input_ids'][0], 
                             bert_inp['token_type_ids'][0], 
                             bert_inp['attention_mask'][0]))

    input_ids = np.asarray(input_ids)
    token_type_ids = np.asarray(token_type_ids)
    attention_mask = np.array(attention_mask)
    
    return input_ids, token_type_ids, attention_mask, bert_inp_tot
    

# Fonction de création des features
def feature_BERT_fct(model, model_type, sentences, max_length, b_size, mode='HF') :
    batch_size = b_size
    batch_size_pred = b_size
    bert_tokenizer = AutoTokenizer.from_pretrained(model_type)
    time1 = time.time()

    for step in range(len(sentences)//batch_size) :
        idx = step*batch_size
        input_ids, token_type_ids, attention_mask, bert_inp_tot = bert_inp_fct(sentences[idx:idx+batch_size], 
                                                                      bert_tokenizer, max_length)
        
        if mode=='HF' :    # Bert HuggingFace
            outputs = model.predict([input_ids, attention_mask, token_type_ids], batch_size=batch_size_pred)
            last_hidden_states = outputs.last_hidden_state

        if mode=='TFhub' : # Bert Tensorflow Hub
            text_preprocessed = {"input_word_ids" : input_ids, 
                                 "input_mask" : attention_mask, 
                                 "input_type_ids" : token_type_ids}
            outputs = model(text_preprocessed)
            last_hidden_states = outputs['sequence_output']
             
        if step ==0 :
            last_hidden_states_tot = last_hidden_states
            last_hidden_states_tot_0 = last_hidden_states
        else :
            last_hidden_states_tot = np.concatenate((last_hidden_states_tot,last_hidden_states))
    
    features_bert = np.array(last_hidden_states_tot).mean(axis=1)
    
    time2 = np.round(time.time() - time1,0)
    print("temps traitement : ", time2)
     
    return features_bert, last_hidden_states_tot

"""## BERT HuggingFace

### 'bert-base-uncased'
"""

max_length = 64
batch_size = 10
model_type = 'bert-base-uncased'
model = TFAutoModel.from_pretrained(model_type)
sentences = df['sentence_dl'].to_list()

# Création des features

features_bert, last_hidden_states_tot = feature_BERT_fct(model, model_type, sentences, 
                                                         max_length, batch_size, mode='HF')

ARI, X_tsne, labels = ARI_fct(features_bert)

TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI)

ari_bert_base_uncased = ARI

import tensorflow_hub as hub
import tensorflow_text 

# Guide sur le Tensorflow hub : https://www.tensorflow.org/text/tutorials/classify_text_with_bert
model_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4'
bert_layer = hub.KerasLayer(model_url, trainable=True)

sentences = df['sentence_dl'].to_list()

max_length = 64
batch_size = 10
model_type = 'bert-base-uncased'
model = bert_layer

features_bert, last_hidden_states_tot = feature_BERT_fct(model, model_type, sentences, 
                                                         max_length, batch_size, mode='TFhub')

ARI, X_tsne, labels = ARI_fct(features_bert)

TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI)

ari_hub_tensorflow = ARI

"""# USE - Universal Sentence Encoder"""

import tensorflow as tf
# import tensorflow_hub as hub
import tensorflow.keras
from tensorflow.keras import backend as K

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import metrics as kmetrics
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model

# Bert
import transformers
from transformers import *

os.environ["TF_KERAS"]='1'

print(tf.__version__)
print(tensorflow.__version__)
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
print(tf.test.is_built_with_cuda())

import tensorflow_hub as hub

embed = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")

def feature_USE_fct(sentences, b_size) :
    batch_size = b_size
    time1 = time.time()

    for step in range(len(sentences)//batch_size) :
        idx = step*batch_size
        feat = embed(sentences[idx:idx+batch_size])

        if step ==0 :
            features = feat
        else :
            features = np.concatenate((features,feat))

    time2 = np.round(time.time() - time1,0)
    return features

batch_size = 10
sentences = df['sentence_dl'].to_list()

features_USE = feature_USE_fct(sentences, batch_size)

ARI, X_tsne, labels = ARI_fct(features_USE)

TSNE_visu_fct(X_tsne, y_cat_num, labels, ARI)

ari_use = ARI

print('ARI BERT base uncased: %s, ARI hub tensorflow: %s, ARI USE: %s' % (ari_bert_base_uncased, ari_hub_tensorflow, ari_use))